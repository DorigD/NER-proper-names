{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path ../models/roberta-finetuned-ner is local\n",
      "Loading tokenizer from ../models/roberta-finetuned-ner\n",
      "Found custom config at ../models/roberta-finetuned-ner\\model_config.json\n",
      "Loading model from ../models/roberta-finetuned-ner\n",
      "Model loaded with 3 labels\n",
      "Creating NER pipeline on device: 0\n",
      "Model architecture: RobertaForTokenClassification\n",
      "Number of parameters: 124057347\n",
      "Labels: {0: 'O', 1: 'B-PERSON', 2: 'I-PERSON'}\n",
      "Tokenizer: RobertaTokenizerFast\n",
      "Vocabulary size: 50265\n",
      "\n",
      "--- Processing with model: ../models/roberta-finetuned-ner ---\n",
      "Input text: Barack Obama was president of the United States. He worked with Joe Biden, who later became president too. joe was a bad President\n",
      "\n",
      "Detected person names:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Barack Obama** was president of the United States. He worked with **Joe Biden**, who later became president too. **joe** was a bad President"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity details:\n",
      "Entity               Type       Confidence\n",
      "--------------------------------------------------\n",
      "Barack Obama         PERSON     0.9995    \n",
      " Joe Biden           PERSON     0.9998    \n",
      " joe                 PERSON     0.9974    \n",
      "\n",
      "Total: 3 person entities detected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "def is_local_model(model_path):\n",
    "    \"\"\"Check if model path refers to a local model directory\"\"\"\n",
    "    return os.path.exists(model_path)\n",
    "\n",
    "def load_model_config(model_path, debug=False):\n",
    "    \"\"\"\n",
    "    Load model configuration from a custom config file if it exists\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model directory\n",
    "        debug: Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration with id2label and label2id mappings\n",
    "    \"\"\"\n",
    "    config_path = os.path.join(model_path, \"model_config.json\")\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        if debug:\n",
    "            print(f\"Found custom config at {config_path}\")\n",
    "        \n",
    "        with open(config_path, \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "        \n",
    "        # Extract label mappings\n",
    "        id2label = model_config.get(\"id2label\", {\"0\": \"O\", \"1\": \"B-PERSON\", \"2\": \"I-PERSON\"})\n",
    "        label2id = model_config.get(\"label2id\", {\"O\": 0, \"B-PERSON\": 1, \"I-PERSON\": 2})\n",
    "        \n",
    "        # Convert string keys to integers for id2label\n",
    "        id2label = {int(k): v for k, v in id2label.items()}\n",
    "        \n",
    "        return {\"id2label\": id2label, \"label2id\": label2id}\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"No custom config found, using default settings\")\n",
    "        \n",
    "        # Default settings for person NER\n",
    "        return {\n",
    "            \"id2label\": {0: \"O\", 1: \"B-PERSON\", 2: \"I-PERSON\"},\n",
    "            \"label2id\": {\"O\": 0, \"B-PERSON\": 1, \"I-PERSON\": 2}\n",
    "        }\n",
    "\n",
    "def load_tokenizer(model_path, debug=False):\n",
    "    \"\"\"Load tokenizer from path\"\"\"\n",
    "    if debug:\n",
    "        print(f\"Loading tokenizer from {model_path}\")\n",
    "    return AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def configure_base_model(model_path, debug=False):\n",
    "    \"\"\"\n",
    "    Check if a model is a base model requiring NER configuration\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path or name of the model\n",
    "        debug: Whether to print debug info\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether this is a base model needing configuration\n",
    "    \"\"\"\n",
    "    is_base = \"base\" in model_path.lower() and \"ner\" not in model_path.lower()\n",
    "    if is_base and debug:\n",
    "        print(f\"Detected {model_path} as base model requiring NER configuration\")\n",
    "    return is_base\n",
    "\n",
    "def load_model(model_path, config=None, debug=False):\n",
    "    \"\"\"\n",
    "    Load model with appropriate configuration\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model\n",
    "        config: Optional configuration dictionary with id2label and label2id\n",
    "        debug: Whether to print debug info\n",
    "    \n",
    "    Returns:\n",
    "        Model: The loaded model\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    # If no config provided, use default for NER\n",
    "    if config is None:\n",
    "        config = {\n",
    "            \"id2label\": {0: \"O\", 1: \"B-PERSON\", 2: \"I-PERSON\"},\n",
    "            \"label2id\": {\"O\": 0, \"B-PERSON\": 1, \"I-PERSON\": 2}\n",
    "        }\n",
    "    \n",
    "    # Load model with configuration\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        id2label=config[\"id2label\"],\n",
    "        label2id=config[\"label2id\"],\n",
    "        num_labels=len(config[\"id2label\"])\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Model loaded with {len(config['id2label'])} labels\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_ner_pipeline(model, tokenizer, debug=False):\n",
    "    \"\"\"\n",
    "    Create NER pipeline from model and tokenizer\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use\n",
    "        tokenizer: The tokenizer to use\n",
    "        debug: Whether to print debug info\n",
    "    \n",
    "    Returns:\n",
    "        pipeline: NER pipeline\n",
    "    \"\"\"\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    if debug:\n",
    "        print(f\"Creating NER pipeline on device: {device}\")\n",
    "    \n",
    "    return pipeline(\n",
    "        \"ner\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        aggregation_strategy=\"simple\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "def test_pipeline(pipeline, test_text=\"Barack Obama was president of the United States.\", debug=False):\n",
    "    \"\"\"\n",
    "    Test NER pipeline on a sample text\n",
    "    \n",
    "    Args:\n",
    "        pipeline: NER pipeline to test\n",
    "        test_text: Sample text for testing\n",
    "        debug: Whether to print full results\n",
    "    \n",
    "    Returns:\n",
    "        list: Detected entities\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Testing pipeline on: '{test_text}'\")\n",
    "    \n",
    "    entities = pipeline(test_text)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Detected entities: {entities}\")\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def print_model_info(model, tokenizer):\n",
    "    \"\"\"Print detailed information about the model\"\"\"\n",
    "    print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    print(f\"Labels: {model.config.id2label}\")\n",
    "    print(f\"Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "def load_and_setup_ner_model(model_path, test=True, debug=False):\n",
    "    \"\"\"\n",
    "    Main function to load and set up an NER model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path or name of model\n",
    "        test: Whether to run a quick test\n",
    "        debug: Whether to print debug info\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model, ner_pipeline)\n",
    "    \"\"\"\n",
    "    # Check if local model\n",
    "    local = is_local_model(model_path)\n",
    "    if debug:\n",
    "        print(f\"Model path {model_path} is {'local' if local else 'from HuggingFace'}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = load_tokenizer(model_path, debug)\n",
    "    \n",
    "    # Determine if base model\n",
    "    is_base = configure_base_model(model_path, debug)\n",
    "    \n",
    "    # Load configuration\n",
    "    if local:\n",
    "        config = load_model_config(model_path, debug)\n",
    "    elif is_base:\n",
    "        config = {\n",
    "            \"id2label\": {0: \"O\", 1: \"B-PERSON\", 2: \"I-PERSON\"},\n",
    "            \"label2id\": {\"O\": 0, \"B-PERSON\": 1, \"I-PERSON\": 2}\n",
    "        }\n",
    "    else:\n",
    "        config = None  # Use model's existing config\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, config, debug)\n",
    "    \n",
    "    # Create pipeline\n",
    "    ner_pipe = create_ner_pipeline(model, tokenizer, debug)\n",
    "    \n",
    "    # Test pipeline\n",
    "    if test:\n",
    "        entities = test_pipeline(ner_pipe, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"Found {len(entities)} entities in test text\")\n",
    "    \n",
    "    # Print detailed model info if requested\n",
    "    if debug:\n",
    "        print_model_info(model, tokenizer)\n",
    "    \n",
    "    return tokenizer, model, ner_pipe\n",
    "\n",
    "# Example usage:\n",
    "# tokenizer, model, pipeline = load_and_setup_ner_model(\"../models/roberta-finetuned-ner\", debug=True)\n",
    "\n",
    "def extract_and_display_names(text, model_path, debug=False):\n",
    "    \"\"\"\n",
    "    Extract person names from text using a specified NER model and display results.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "        model_path (str): Path or name of NER model\n",
    "        debug (bool): Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "        list: Detected entities\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    a, b, ner_pipeline = load_and_setup_ner_model(model_path, test=False, debug=debug)\n",
    "    \n",
    "    print(f\"\\n--- Processing with model: {model_path} ---\")\n",
    "    print(f\"Input text: {text}\")\n",
    "    \n",
    "    # Get entities\n",
    "    entities = ner_pipeline(text)\n",
    "    \n",
    "    # Filter for person entities\n",
    "    person_entities = [e for e in entities if e[\"entity_group\"] == \"PERSON\"]\n",
    "    \n",
    "    # Display results\n",
    "    if not person_entities:\n",
    "        print(\"No person names detected.\")\n",
    "        return entities\n",
    "    \n",
    "    # Create highlighted text display with markdown formatting\n",
    "    highlighted_text = text\n",
    "    # Sort entities by start position in reverse to avoid offset issues\n",
    "    for entity in sorted(person_entities, key=lambda x: x[\"start\"], reverse=True):\n",
    "        start, end = entity[\"start\"], entity[\"end\"]\n",
    "        entity_text = text[start:end]\n",
    "        highlighted_text = highlighted_text[:start] + f\"**{entity_text}**\" + highlighted_text[end:]\n",
    "    \n",
    "    print(\"\\nDetected person names:\")\n",
    "    print(\"-\" * 40)\n",
    "    from IPython.display import Markdown, display\n",
    "    display(Markdown(highlighted_text))\n",
    "    \n",
    "    # Print entity details in a table\n",
    "    print(\"\\nEntity details:\")\n",
    "    print(f\"{'Entity':<20} {'Type':<10} {'Confidence':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for entity in person_entities:\n",
    "        name = entity[\"word\"]\n",
    "        confidence = f\"{entity['score']:.4f}\"\n",
    "        print(f\"{name:<20} {'PERSON':<10} {confidence:<10}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(person_entities)} person entities detected\")\n",
    "    return entities\n",
    "\n",
    "entities = extract_and_display_names(\n",
    "\"Barack Obama was president of the United States. He worked with Joe Biden, who later became president too. joe was a bad President\",\n",
    "    \"../models/roberta-finetuned-ner\", debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
