{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation results:\n",
      "  eval_loss: 0.008307608775794506\n",
      "  eval_accuracy: 0.9984569643596403\n",
      "  eval_precision: 0.9984569643596403\n",
      "  eval_recall: 0.9984569643596403\n",
      "  eval_f1: 0.9984569643596403\n",
      "  eval_person_precision: 0.9901873327386262\n",
      "  eval_person_recall: 0.9910714285714286\n",
      "  eval_person_f1: 0.9906291834002677\n",
      "  eval_runtime: 4.4469\n",
      "  eval_samples_per_second: 674.17\n",
      "  eval_steps_per_second: 42.276\n",
      "  epoch: 5.0\n",
      "Input text:\n",
      "Barack Obama was the 44th president of the United States, and he worked with Joe Biden as his vice president. Later, Kamala Harris became vice president under President Biden.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Found 4 person names:\n",
      "1. Barack Obama\n",
      "2. Joe Biden\n",
      "3. Kamala Harris\n",
      "4. Biden\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Text with highlighted names:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Barack Obama** was the 44th president of the United States, and he worked with **Joe Biden** as his vice president. Later, **Kamala Harris** became vice president under President **Biden**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with simple text: John Smith is here.\n",
      "\n",
      "DEBUG INFO:\n",
      "<s>: O\n",
      "ĠJohn: B-PERSON\n",
      "ĠSmith: I-PERSON\n",
      "Ġis: O\n",
      "Ġhere: O\n",
      ".: O\n",
      "</s>: O\n",
      "Found names: ['John Smith']\n",
      "Training set size: 11989\n",
      "Test set size: 2998\n",
      "Label distribution in training data:\n",
      "{'O': 194182, 'B-PERSON': 5253, 'I-PERSON': 12036}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Define paths to load the model\n",
    "project_dir = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "model_dir = os.path.join(project_dir, \"models\", \"roberta-finetuned-ner\")\n",
    "\n",
    "# Load model configuration\n",
    "with open(os.path.join(model_dir, \"model_config.json\"), \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, add_prefix_space=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Get id2label mapping\n",
    "id2label = {int(k): v for k, v in model_config[\"id2label\"].items()}\n",
    "def extract_names(text, debug=False):\n",
    "    \"\"\"\n",
    "    Extract person names from text using the trained NER model\n",
    "    \"\"\"\n",
    "    # Tokenize with the same parameters as during training\n",
    "    # REMOVE add_prefix_space=True from here since it's already set in tokenizer initialization\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True, \n",
    "                      return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0].numpy()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=2)\n",
    "        predictions = torch.argmax(logits, dim=2)[0].numpy()\n",
    "    \n",
    "    # Extract tokens and their predictions\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    token_predictions = [id2label[pred] for pred in predictions]\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nDEBUG INFO:\")\n",
    "        for i, (token, pred) in enumerate(zip(tokens, token_predictions)):\n",
    "            print(f\"{token}: {pred}\")\n",
    "    \n",
    "    # Extract full names by merging adjacent tokens\n",
    "    names = []\n",
    "    current_name = []\n",
    "    current_start = None\n",
    "    \n",
    "    # Process each token\n",
    "    for idx, (token, pred, offset) in enumerate(zip(tokens, token_predictions, offset_mapping)):\n",
    "        # Skip special tokens\n",
    "        if offset[0] == offset[1]:\n",
    "            continue\n",
    "            \n",
    "        if pred == \"B-PERSON\":\n",
    "            # If we were building a name, save it\n",
    "            if current_name:\n",
    "                names.append({\n",
    "                    \"name\": text[current_start:offset_mapping[idx-1][1]].strip(),\n",
    "                    \"start\": current_start,\n",
    "                    \"end\": offset_mapping[idx-1][1]\n",
    "                })\n",
    "                current_name = []\n",
    "            \n",
    "            # Start new name\n",
    "            current_name.append(token)\n",
    "            current_start = offset[0]\n",
    "            \n",
    "        elif pred == \"I-PERSON\" and current_name:\n",
    "            # Continue building name\n",
    "            current_name.append(token)\n",
    "            \n",
    "        else:\n",
    "            # End of a name\n",
    "            if current_name:\n",
    "                names.append({\n",
    "                    \"name\": text[current_start:offset_mapping[idx-1][1]].strip(),\n",
    "                    \"start\": current_start,\n",
    "                    \"end\": offset_mapping[idx-1][1]\n",
    "                })\n",
    "                current_name = []\n",
    "                current_start = None\n",
    "    \n",
    "    # Handle case where name ends at the end of the text\n",
    "    if current_name and len(offset_mapping) > 0:\n",
    "        names.append({\n",
    "            \"name\": text[current_start:offset_mapping[-1][1]].strip(),\n",
    "            \"start\": current_start,\n",
    "            \"end\": offset_mapping[-1][1]\n",
    "        })\n",
    "    \n",
    "    return names\n",
    "\n",
    "\n",
    "def highlight_names_in_text(text, names):\n",
    "    \"\"\"\n",
    "    Highlight detected names in the original text for display\n",
    "    \n",
    "    Args:\n",
    "        text (str): Original text\n",
    "        names (list): List of name objects with start/end positions\n",
    "        \n",
    "    Returns:\n",
    "        str: Markdown formatted text with highlighted names\n",
    "    \"\"\"\n",
    "    # Sort names by start position (descending)\n",
    "    sorted_names = sorted(names, key=lambda x: x[\"start\"], reverse=True)\n",
    "    \n",
    "    # Insert markdown formatting\n",
    "    highlighted_text = text\n",
    "    for name in sorted_names:\n",
    "        prefix = highlighted_text[:name[\"start\"]]\n",
    "        name_text = highlighted_text[name[\"start\"]:name[\"end\"]]\n",
    "        suffix = highlighted_text[name[\"end\"]:]\n",
    "        highlighted_text = f\"{prefix}**{name_text}**{suffix}\"\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "# Demo function\n",
    "def analyze_text(text):\n",
    "    \"\"\"\n",
    "    Analyze text and display the results with highlighted names\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \"\"\"\n",
    "    print(\"Input text:\")\n",
    "    print(text)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Extract names\n",
    "    names = extract_names(text)\n",
    "    \n",
    "    # Display results\n",
    "    if names:\n",
    "        print(f\"Found {len(names)} person names:\")\n",
    "        for i, name in enumerate(names, 1):\n",
    "            print(f\"{i}. {name['name']}\")\n",
    "        \n",
    "        # Display highlighted text\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        print(\"Text with highlighted names:\")\n",
    "        highlighted = highlight_names_in_text(text, names)\n",
    "        display(Markdown(highlighted))\n",
    "    else:\n",
    "        print(\"No person names detected in the text.\")\n",
    "\n",
    "# Check if the model was trained correctly\n",
    "def check_model():\n",
    "    # Load evaluation results\n",
    "    try:\n",
    "        eval_path = os.path.join(model_dir, \"eval_results.json\")\n",
    "        with open(eval_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        print(\"Model evaluation results:\")\n",
    "        for k, v in results.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No evaluation results found. Model may not be properly trained.\")\n",
    "\n",
    "check_model()\n",
    "# Example usage\n",
    "sample_text = \"Barack Obama was the 44th president of the United States, and he worked with Joe Biden as his vice president. Later, Kamala Harris became vice president under President Biden.\"\n",
    "analyze_text(sample_text)\n",
    "\n",
    "def test_simple():\n",
    "    # Try with a very simple example\n",
    "    simple_text = \"John Smith is here.\"\n",
    "    print(\"\\nTesting with simple text:\", simple_text)\n",
    "    names = extract_names(simple_text, debug=True)\n",
    "    print(f\"Found names: {[name['name'] for name in names]}\")\n",
    "\n",
    "test_simple()\n",
    "\n",
    "# Add to your notebook\n",
    "def check_training_data():\n",
    "    try:\n",
    "        # Path to your dataset\n",
    "        dataset_path = os.path.join(project_dir, \"data\", \"tokenized_train\")\n",
    "        from datasets import load_from_disk\n",
    "        dataset = load_from_disk(dataset_path)\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"Training set size: {len(dataset['train'])}\")\n",
    "        print(f\"Test set size: {len(dataset['test'])}\")\n",
    "        \n",
    "        # Check label distribution in training data\n",
    "        label_count = {\"O\": 0, \"B-PERSON\": 0, \"I-PERSON\": 0}\n",
    "        for example in dataset['train']:\n",
    "            for label in example['labels']:\n",
    "                if label == 0:\n",
    "                    label_count[\"O\"] += 1\n",
    "                elif label == 1:\n",
    "                    label_count[\"B-PERSON\"] += 1\n",
    "                elif label == 2:\n",
    "                    label_count[\"I-PERSON\"] += 1\n",
    "                    \n",
    "        print(\"Label distribution in training data:\")\n",
    "        print(label_count)\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking training data: {str(e)}\")\n",
    "\n",
    "check_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from your custom model:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Results from Hugging Face NER pipeline:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- John (score: 0.9995)\n",
      "- Smith (score: 0.9996)\n",
      "- Sarah (score: 0.9993)\n",
      "- Johnson (score: 0.9997)\n",
      "- Williams (score: 0.9989)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def compare_models(text):\n",
    "    \"\"\"\n",
    "    Compare your model with other pretrained NER models\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "    \"\"\"\n",
    "    # Your model\n",
    "    print(\"Results from your custom model:\")\n",
    "    names = extract_names(text)\n",
    "    for name in names:\n",
    "        print(f\"- {name['name']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Hugging Face NER pipeline\n",
    "    print(\"Results from Hugging Face NER pipeline:\")\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"../models/roberta-finetuned-ner\", tokenizer=\"../models/roberta-finetuned-ner\")\n",
    "    results = ner_pipeline(text)\n",
    "    \n",
    "    # Filter for person entities\n",
    "    person_entities = [entity for entity in results if entity[\"entity\"].endswith(\"PER\")]\n",
    "    for entity in person_entities:\n",
    "        print(f\"- {entity['word']} (score: {entity['score']:.4f})\")\n",
    "\n",
    "# Example comparison\n",
    "sample_text = \"When John Smith met Sarah Johnson at the conference, they were introduced by Professor Williams.\"\n",
    "compare_models(sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
