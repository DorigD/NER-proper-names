{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423f7f4a",
   "metadata": {},
   "source": [
    "# NER Model Evaluation Pipeline\n",
    "\n",
    "This notebook provides a comprehensive evaluation pipeline for testing version 2 NER models on TITLE and NO-TITLE datasets.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Models**: roberta-finetuned-ner-TITLE-v2 and roberta-finetuned-ner-NO-TITLE-v2\n",
    "2. **Datasets**: All transformer datasets in `/data/ds/TITLE/` and `/data/ds/NO-TITLE/`\n",
    "3. **Metrics**: F1 score, Precision, Recall, Token Accuracy, Inference Time, Model Size\n",
    "4. **Output**: Aggregated results saved to CSV and JSON for charting\n",
    "\n",
    "## Results:\n",
    "- TITLE model performance on TITLE datasets\n",
    "- NO-TITLE model performance on NO-TITLE datasets\n",
    "- Aggregated metrics for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ed1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "# Add project root to path for importing custom modules\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import custom model and functions\n",
    "from scripts.train import (\n",
    "    RobertaCRFForTokenClassification, \n",
    "    confidence_based_postprocessing,\n",
    "    LABEL2ID, ID2LABEL, NUM_LABELS\n",
    ")\n",
    "\n",
    "print(\" All libraries imported successfully!\")\n",
    "print(f\" PyTorch version: {torch.__version__}\")\n",
    "print(f\" Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Paths\n",
    "from utils.config import PROJECT_DIR\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = PROJECT_DIR\n",
    "DATA_DS_PATH = os.path.join(PROJECT_ROOT, \"data\", \"ds\")\n",
    "MODELS_PATH = os.path.join(PROJECT_ROOT, \"models\")\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, \"evaluation_results\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Model paths\n",
    "TITLE_MODEL_PATH = os.path.join(MODELS_PATH, \"roberta-finetuned-ner-TITLE-v2\")\n",
    "NO_TITLE_MODEL_PATH = os.path.join(MODELS_PATH, \"roberta-finetuned-ner-NO-TITLE-v2\")\n",
    "\n",
    "# Dataset paths\n",
    "TITLE_DATASETS_PATH = os.path.join(DATA_DS_PATH, \"TITLE\")\n",
    "NO_TITLE_DATASETS_PATH = os.path.join(DATA_DS_PATH, \"NO-TITLE\")\n",
    "\n",
    "print(\" Configuration:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  TITLE Model: {TITLE_MODEL_PATH}\")\n",
    "print(f\"  NO-TITLE Model: {NO_TITLE_MODEL_PATH}\")\n",
    "print(f\"  TITLE Datasets: {TITLE_DATASETS_PATH}\")\n",
    "print(f\"  NO-TITLE Datasets: {NO_TITLE_DATASETS_PATH}\")\n",
    "print(f\"  Results Output: {RESULTS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "for path, name in [(TITLE_MODEL_PATH, \"TITLE Model\"), (NO_TITLE_MODEL_PATH, \"NO-TITLE Model\"), \n",
    "                   (TITLE_DATASETS_PATH, \"TITLE Datasets\"), (NO_TITLE_DATASETS_PATH, \"NO-TITLE Datasets\")]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\" {name} found\")\n",
    "    else:\n",
    "        print(f\" {name} NOT found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70442c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Discovery Function\n",
    "def discover_datasets(datasets_path):\n",
    "    \"\"\"\n",
    "    Discover all transformer datasets in the given directory.\n",
    "    Returns a list of dataset paths.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    datasets_path = Path(datasets_path)\n",
    "    \n",
    "    if not datasets_path.exists():\n",
    "        print(f\" Path {datasets_path} does not exist!\")\n",
    "        return datasets\n",
    "    \n",
    "    # Look for directories that contain dataset files\n",
    "    for item in datasets_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            # Check if it's a valid transformers dataset\n",
    "            if (item / \"dataset_dict.json\").exists() or (item / \"dataset_info.json\").exists():\n",
    "                datasets.append(item)\n",
    "                print(f\" Found dataset: {item.name}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Discover all datasets\n",
    "print(\" Discovering TITLE datasets...\")\n",
    "title_datasets = discover_datasets(TITLE_DATASETS_PATH)\n",
    "\n",
    "print(f\"\\n Discovering NO-TITLE datasets...\")\n",
    "no_title_datasets = discover_datasets(NO_TITLE_DATASETS_PATH)\n",
    "\n",
    "print(f\"\\n Summary:\")\n",
    "print(f\"  TITLE datasets: {len(title_datasets)}\")\n",
    "print(f\"  NO-TITLE datasets: {len(no_title_datasets)}\")\n",
    "print(f\"  Total datasets: {len(title_datasets) + len(no_title_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66640937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "# Model Loading Functions\n",
    "def load_model_and_tokenizer(model_path, model_type=\"NO-TITLE\"):\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer from the given path.\n",
    "    Returns model, tokenizer, label_config\n",
    "    \"\"\"\n",
    "    print(f\" Loading {model_type} model from {model_path}\")\n",
    "    \n",
    "    # Convert to Path object if it's a string\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    # Load label configuration\n",
    "    label_config_path = model_path / \"label_config.json\"\n",
    "    with open(label_config_path, 'r') as f:\n",
    "        label_config = json.load(f)\n",
    "    \n",
    "    # Get model configuration from config.json\n",
    "    config_path = model_path / \"config.json\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Create model with exact same architecture as training\n",
    "    model = RobertaCRFForTokenClassification(\n",
    "        model_name=\"roberta-base\",  # Base model\n",
    "        num_labels=label_config['num_labels'],\n",
    "        alpha=config.get('alpha', 0.25),\n",
    "        gamma=config.get('gamma', 2.0),\n",
    "        person_weight=config.get('person_weight', 5.0),\n",
    "        crf_weight=config.get('crf_weight', 0.5),\n",
    "        focal_weight=config.get('focal_weight', 0.2),\n",
    "        dice_weight=config.get('dice_weight', 0.3),\n",
    "        classifier_params=config.get('classifier_params', {}),\n",
    "        dice_loss_params=config.get('dice_loss_params', {})\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    from safetensors.torch import load_file\n",
    "    model_file = model_path / \"model.safetensors\"\n",
    "    if model_file.exists():\n",
    "        state_dict = load_file(str(model_file))\n",
    "        \n",
    "        # Filter compatible weights\n",
    "        model_state_dict = model.state_dict()\n",
    "        compatible_state_dict = {}\n",
    "        \n",
    "        for key, value in state_dict.items():\n",
    "            if key in model_state_dict and model_state_dict[key].shape == value.shape:\n",
    "                compatible_state_dict[key] = value\n",
    "        \n",
    "        model.load_state_dict(compatible_state_dict, strict=False)\n",
    "        print(f\"   Loaded {len(compatible_state_dict)} weight tensors\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path), add_prefix_space=True)\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "    print(f\"   Model size: {model_size_mb:.1f} MB\")\n",
    "    \n",
    "    return model, tokenizer, label_config, model_size_mb\n",
    "\n",
    "# Load both models\n",
    "print(\"Loading models...\")\n",
    "title_model, title_tokenizer, title_label_config, title_model_size = load_model_and_tokenizer(\n",
    "    TITLE_MODEL_PATH, \"TITLE\"\n",
    ")\n",
    "\n",
    "no_title_model, no_title_tokenizer, no_title_label_config, no_title_model_size = load_model_and_tokenizer(\n",
    "    NO_TITLE_MODEL_PATH, \"NO-TITLE\"\n",
    ")\n",
    "\n",
    "print(f\"\\n Both models loaded successfully!\")\n",
    "print(f\"  TITLE model: {title_label_config['num_labels']} labels, {title_model_size:.1f} MB\")\n",
    "print(f\"  NO-TITLE model: {no_title_label_config['num_labels']} labels, {no_title_model_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1257d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model_on_dataset(model, tokenizer, label_config, dataset_path, model_type):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a single dataset and return metrics.\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_path.name\n",
    "    print(f\"   Evaluating on {dataset_name}...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        dataset = load_from_disk(str(dataset_path))\n",
    "        \n",
    "        # Check if dataset has splits and use appropriate data\n",
    "        if hasattr(dataset, 'keys'):\n",
    "            # Dataset has splits, use test if available, otherwise use the first available split\n",
    "            if 'test' in dataset:\n",
    "                eval_data = dataset['test']\n",
    "            elif 'validation' in dataset:\n",
    "                eval_data = dataset['validation']\n",
    "            else:\n",
    "                eval_data = dataset[list(dataset.keys())[0]]\n",
    "        else:\n",
    "            # Single dataset without splits\n",
    "            eval_data = dataset\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"     Failed to load dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Filter labels to match model\n",
    "    def filter_labels(example):\n",
    "        filtered_labels = []\n",
    "        for label in example['labels']:\n",
    "            if label == -100:\n",
    "                filtered_labels.append(label)\n",
    "            elif label >= label_config['num_labels']:\n",
    "                filtered_labels.append(0)  # Map invalid labels to O\n",
    "            else:\n",
    "                filtered_labels.append(label)\n",
    "        example['labels'] = filtered_labels\n",
    "        return example\n",
    "    \n",
    "    eval_data = eval_data.map(filter_labels)\n",
    "    \n",
    "    # Set up evaluation\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process in batches\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(eval_data, batch_size=8, collate_fn=data_collator)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=None)\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            # Use CRF decoding\n",
    "            try:\n",
    "                crf_mask = attention_mask.bool()\n",
    "                if hasattr(model, 'crf') and hasattr(model.crf, 'decode'):\n",
    "                    crf_predictions = model.crf.decode(logits, mask=crf_mask)\n",
    "                    predictions = torch.zeros_like(input_ids)\n",
    "                    for b_idx, pred_seq in enumerate(crf_predictions):\n",
    "                        seq_len = min(len(pred_seq), predictions.shape[1])\n",
    "                        predictions[b_idx, :seq_len] = torch.tensor(pred_seq[:seq_len])\n",
    "                else:\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "            except:\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Apply post-processing\n",
    "            try:\n",
    "                predictions = confidence_based_postprocessing(logits, predictions, attention_mask)\n",
    "            except:\n",
    "                pass  # Continue with unprocessed predictions\n",
    "            \n",
    "            # Extract sequences for evaluation\n",
    "            for b in range(predictions.shape[0]):\n",
    "                pred_seq = []\n",
    "                label_seq = []\n",
    "                \n",
    "                for t in range(predictions.shape[1]):\n",
    "                    if attention_mask[b, t] == 1 and labels[b, t] != -100:\n",
    "                        pred_id = predictions[b, t].item()\n",
    "                        label_id = labels[b, t].item()\n",
    "                        \n",
    "                        pred_label = label_config['id2label'].get(str(pred_id), \"O\")\n",
    "                        true_label = label_config['id2label'].get(str(label_id), \"O\")\n",
    "                        \n",
    "                        pred_seq.append(pred_label)\n",
    "                        label_seq.append(true_label)\n",
    "                \n",
    "                if pred_seq and label_seq:\n",
    "                    all_predictions.append(pred_seq)\n",
    "                    all_labels.append(label_seq)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if not all_predictions or not all_labels:\n",
    "        print(f\"     No valid predictions for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Entity-level metrics using seqeval\n",
    "    entity_results = seq_classification_report(all_labels, all_predictions, scheme=IOB2, output_dict=True)\n",
    "    \n",
    "    # Token-level metrics\n",
    "    all_true_labels = [l for seq in all_labels for l in seq]\n",
    "    all_pred_labels = [p for seq in all_predictions for p in seq]\n",
    "    \n",
    "    token_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # Get precision, recall, F1 for PERSON entity specifically\n",
    "    person_metrics = entity_results.get(\"PERSON\", {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0})\n",
    "    \n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'model_type': model_type,\n",
    "        'num_samples': len(all_predictions),\n",
    "        'inference_time_seconds': inference_time,\n",
    "        'inference_time_per_sample': inference_time / len(all_predictions),\n",
    "        'person_precision': person_metrics[\"precision\"],\n",
    "        'person_recall': person_metrics[\"recall\"],\n",
    "        'person_f1': person_metrics[\"f1-score\"],\n",
    "        'entity_f1_macro': entity_results[\"macro avg\"][\"f1-score\"],\n",
    "        'token_accuracy': token_accuracy,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"     {dataset_name}: Person F1={results['person_f1']:.3f}, Token Acc={results['token_accuracy']:.3f}, Time={inference_time:.1f}s\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581392e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Evaluation Pipeline\n",
    "print(\" Starting comprehensive evaluation pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Evaluate TITLE model on TITLE datasets\n",
    "print(\"\\n TITLE Model Evaluation\")\n",
    "print(\"-\" * 30)\n",
    "for dataset_path in title_datasets:\n",
    "    result = evaluate_model_on_dataset(\n",
    "        title_model, title_tokenizer, title_label_config, \n",
    "        dataset_path, \"TITLE\"\n",
    "    )\n",
    "    if result:\n",
    "        result['model_size_mb'] = title_model_size\n",
    "        all_results.append(result)\n",
    "\n",
    "# Evaluate NO-TITLE model on NO-TITLE datasets  \n",
    "print(\"\\n NO-TITLE Model Evaluation\")\n",
    "print(\"-\" * 30)\n",
    "for dataset_path in no_title_datasets:\n",
    "    result = evaluate_model_on_dataset(\n",
    "        no_title_model, no_title_tokenizer, no_title_label_config,\n",
    "        dataset_path, \"NO-TITLE\"\n",
    "    )\n",
    "    if result:\n",
    "        result['model_size_mb'] = no_title_model_size\n",
    "        all_results.append(result)\n",
    "\n",
    "print(f\"\\n Evaluation complete! Processed {len(all_results)} dataset-model combinations.\")\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(f\"\\n Results Summary:\")\n",
    "print(f\"  Total evaluations: {len(df_results)}\")\n",
    "print(f\"  TITLE evaluations: {len(df_results[df_results['model_type'] == 'TITLE'])}\")\n",
    "print(f\"  NO-TITLE evaluations: {len(df_results[df_results['model_type'] == 'NO-TITLE'])}\")\n",
    "\n",
    "# Display first few results\n",
    "if len(df_results) > 0:\n",
    "    print(f\"\\n Sample Results:\")\n",
    "    display_cols = ['dataset', 'model_type', 'person_f1', 'person_precision', 'person_recall', 'token_accuracy']\n",
    "    print(df_results[display_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Results by Model Type\n",
    "print(\"\\n Aggregating Results by Model Type\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Aggregate metrics by model type\n",
    "    aggregated_results = []\n",
    "    \n",
    "    for model_type in ['TITLE', 'NO-TITLE']:\n",
    "        model_data = df_results[df_results['model_type'] == model_type]\n",
    "        \n",
    "        if len(model_data) > 0:\n",
    "            # Calculate aggregate statistics\n",
    "            agg_result = {\n",
    "                'model_type': model_type,\n",
    "                'num_datasets': len(model_data),\n",
    "                'total_samples': model_data['num_samples'].sum(),\n",
    "                'total_inference_time': model_data['inference_time_seconds'].sum(),\n",
    "                'avg_inference_time_per_sample': model_data['inference_time_per_sample'].mean(),\n",
    "                'model_size_mb': model_data['model_size_mb'].iloc[0],  # Same for all entries\n",
    "                \n",
    "                # Person entity metrics\n",
    "                'person_f1_mean': model_data['person_f1'].mean(),\n",
    "                'person_f1_std': model_data['person_f1'].std(),\n",
    "                'person_f1_min': model_data['person_f1'].min(),\n",
    "                'person_f1_max': model_data['person_f1'].max(),\n",
    "                \n",
    "                'person_precision_mean': model_data['person_precision'].mean(),\n",
    "                'person_precision_std': model_data['person_precision'].std(),\n",
    "                \n",
    "                'person_recall_mean': model_data['person_recall'].mean(),\n",
    "                'person_recall_std': model_data['person_recall'].std(),\n",
    "                \n",
    "                # Overall metrics\n",
    "                'entity_f1_macro_mean': model_data['entity_f1_macro'].mean(),\n",
    "                'entity_f1_macro_std': model_data['entity_f1_macro'].std(),\n",
    "                \n",
    "                'token_accuracy_mean': model_data['token_accuracy'].mean(),\n",
    "                'token_accuracy_std': model_data['token_accuracy'].std(),\n",
    "                \n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            aggregated_results.append(agg_result)\n",
    "            \n",
    "            # Print summary for this model type\n",
    "            print(f\"\\n {model_type} Model Summary:\")\n",
    "            print(f\"  Datasets evaluated: {agg_result['num_datasets']}\")\n",
    "            print(f\"  Total samples: {agg_result['total_samples']:,}\")\n",
    "            print(f\"  Model size: {agg_result['model_size_mb']:.1f} MB\")\n",
    "            print(f\"  Total inference time: {agg_result['total_inference_time']:.1f}s\")\n",
    "            print(f\"  Avg time per sample: {agg_result['avg_inference_time_per_sample']*1000:.1f}ms\")\n",
    "            print(f\"  Person F1: {agg_result['person_f1_mean']:.3f} ± {agg_result['person_f1_std']:.3f} (range: {agg_result['person_f1_min']:.3f} - {agg_result['person_f1_max']:.3f})\")\n",
    "            print(f\"  Person Precision: {agg_result['person_precision_mean']:.3f} ± {agg_result['person_precision_std']:.3f}\")\n",
    "            print(f\"  Person Recall: {agg_result['person_recall_mean']:.3f} ± {agg_result['person_recall_std']:.3f}\")\n",
    "            print(f\"  Token Accuracy: {agg_result['token_accuracy_mean']:.3f} ± {agg_result['token_accuracy_std']:.3f}\")\n",
    "    \n",
    "    # Create aggregated DataFrame\n",
    "    df_aggregated = pd.DataFrame(aggregated_results)\n",
    "    \n",
    "    # Comparison summary\n",
    "    if len(df_aggregated) == 2:\n",
    "        print(f\"\\n Model Comparison:\")\n",
    "        title_row = df_aggregated[df_aggregated['model_type'] == 'TITLE'].iloc[0]\n",
    "        no_title_row = df_aggregated[df_aggregated['model_type'] == 'NO-TITLE'].iloc[0]\n",
    "        \n",
    "        print(f\"  F1 Score: TITLE({title_row['person_f1_mean']:.3f}) vs NO-TITLE({no_title_row['person_f1_mean']:.3f})\")\n",
    "        print(f\"  Precision: TITLE({title_row['person_precision_mean']:.3f}) vs NO-TITLE({no_title_row['person_precision_mean']:.3f})\")\n",
    "        print(f\"  Recall: TITLE({title_row['person_recall_mean']:.3f}) vs NO-TITLE({no_title_row['person_recall_mean']:.3f})\")\n",
    "        print(f\"  Speed: TITLE({title_row['avg_inference_time_per_sample']*1000:.1f}ms) vs NO-TITLE({no_title_row['avg_inference_time_per_sample']*1000:.1f}ms)\")\n",
    "        print(f\"  Size: TITLE({title_row['model_size_mb']:.1f}MB) vs NO-TITLE({no_title_row['model_size_mb']:.1f}MB)\")\n",
    "        \n",
    "else:\n",
    "    print(\" No results to aggregate!\")\n",
    "    df_aggregated = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results to Files\n",
    "print(\"\\n Saving Results to Files\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Save detailed results only (aggregated can be calculated when loading)\n",
    "    detailed_csv_path = RESULTS_PATH / f\"detailed_evaluation_results_{timestamp}.csv\"\n",
    "    detailed_json_path = RESULTS_PATH / f\"detailed_evaluation_results_{timestamp}.json\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_results.to_csv(detailed_csv_path, index=False)\n",
    "    print(f\" Detailed results saved to: {detailed_csv_path}\")\n",
    "    \n",
    "    # Save to JSON (with better formatting)\n",
    "    df_results.to_json(detailed_json_path, orient='records', indent=2)\n",
    "    print(f\" Detailed results saved to: {detailed_json_path}\")\n",
    "    \n",
    "    # Save a summary report\n",
    "    summary_path = RESULTS_PATH / f\"evaluation_summary_{timestamp}.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"NER Model Evaluation Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Evaluations: {len(df_results)}\\n\")\n",
    "        f.write(f\"TITLE Evaluations: {len(df_results[df_results['model_type'] == 'TITLE'])}\\n\")\n",
    "        f.write(f\"NO-TITLE Evaluations: {len(df_results[df_results['model_type'] == 'NO-TITLE'])}\\n\\n\")\n",
    "        \n",
    "        # Write individual dataset results summary\n",
    "        f.write(\"Individual Dataset Results:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for _, row in df_results.iterrows():\n",
    "            f.write(f\"{row['dataset']} ({row['model_type']}):\\n\")\n",
    "            f.write(f\"  Person F1: {row['person_f1']:.3f}\\n\")\n",
    "            f.write(f\"  Person Precision: {row['person_precision']:.3f}\\n\")\n",
    "            f.write(f\"  Person Recall: {row['person_recall']:.3f}\\n\")\n",
    "            f.write(f\"  Token Accuracy: {row['token_accuracy']:.3f}\\n\")\n",
    "            f.write(f\"  Inference Time: {row['inference_time_per_sample']*1000:.1f}ms per sample\\n\\n\")\n",
    "        \n",
    "        # Write aggregated statistics (calculated on-the-fly)\n",
    "        if len(df_aggregated) > 0:\n",
    "            f.write(\"\\nAggregated Statistics (calculated from detailed results):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            for _, row in df_aggregated.iterrows():\n",
    "                f.write(f\"{row['model_type']} Model Summary:\\n\")\n",
    "                f.write(f\"  Datasets: {row['num_datasets']}\\n\")\n",
    "                f.write(f\"  Samples: {row['total_samples']:,}\\n\")\n",
    "                f.write(f\"  Person F1: {row['person_f1_mean']:.3f} ± {row['person_f1_std']:.3f}\\n\")\n",
    "                f.write(f\"  Person Precision: {row['person_precision_mean']:.3f} ± {row['person_precision_std']:.3f}\\n\")\n",
    "                f.write(f\"  Person Recall: {row['person_recall_mean']:.3f} ± {row['person_recall_std']:.3f}\\n\")\n",
    "                f.write(f\"  Token Accuracy: {row['token_accuracy_mean']:.3f} ± {row['token_accuracy_std']:.3f}\\n\")\n",
    "                f.write(f\"  Model Size: {row['model_size_mb']:.1f} MB\\n\")\n",
    "                f.write(f\"  Avg Inference Time: {row['avg_inference_time_per_sample']*1000:.1f}ms per sample\\n\\n\")\n",
    "    \n",
    "    print(f\" Summary report saved to: {summary_path}\")\n",
    "    \n",
    "    # Display file list\n",
    "    print(f\"\\n Generated files:\")\n",
    "    for file_path in RESULTS_PATH.glob(f\"*{timestamp}*\"):\n",
    "        print(f\"  {file_path.name}\")\n",
    "        \n",
    "    print(f\"\\n Note: Only detailed results are saved to files.\")\n",
    "    print(f\"   Aggregated statistics can be recalculated when loading the detailed CSV/JSON.\")\n",
    "        \n",
    "else:\n",
    "    print(\" No results to save!\")\n",
    "\n",
    "print(f\"\\n Evaluation pipeline completed successfully!\")\n",
    "print(f\" Results available in: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bde513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Data Preview for Charting\n",
    "print(\"\\\\n Data Preview for Charting\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    print(\"\\\\n Detailed Results Preview:\")\n",
    "    print(\"Columns:\", list(df_results.columns))\n",
    "    print(\"\\\\nSample data:\")\n",
    "    preview_cols = ['dataset', 'model_type', 'person_f1', 'person_precision', 'person_recall', \n",
    "                   'token_accuracy', 'inference_time_per_sample', 'model_size_mb']\n",
    "    print(df_results[preview_cols].head())\n",
    "    \n",
    "    if len(df_aggregated) > 0:\n",
    "        print(\"\\\\n\\\\n Aggregated Results Preview:\")\n",
    "        print(\"Columns:\", list(df_aggregated.columns))\n",
    "        print(\"\\\\nData:\")\n",
    "        agg_preview_cols = ['model_type', 'num_datasets', 'person_f1_mean', 'person_f1_std',\n",
    "                           'person_precision_mean', 'person_recall_mean', 'token_accuracy_mean',\n",
    "                           'model_size_mb', 'avg_inference_time_per_sample']\n",
    "        print(df_aggregated[agg_preview_cols])\n",
    "        \n",
    "    print(f\"\\\\n Ready for Charting!\")\n",
    "    print(f\"   Use 'df_results' for detailed dataset-level analysis\")\n",
    "    print(f\"   Use 'df_aggregated' for model-level comparisons\")\n",
    "    print(f\"   Data also saved as CSV files for external tools\")\n",
    "    \n",
    "else:\n",
    "    print(\" No data available for charting!\")\n",
    "\n",
    "print(f\"\\\\n🏁 Pipeline Complete! Ready for visualization and analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
