{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423f7f4a",
   "metadata": {},
   "source": [
    "# NER Model Evaluation Pipeline\n",
    "\n",
    "This notebook provides a comprehensive evaluation pipeline for testing version 2 NER models on TITLE and NO-TITLE datasets.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Models**: roberta-finetuned-ner-TITLE-v2 and roberta-finetuned-ner-NO-TITLE-v2\n",
    "2. **Datasets**: All transformer datasets in `/data/ds/TITLE/` and `/data/ds/NO-TITLE/`\n",
    "3. **Metrics**: F1 score, Precision, Recall, Token Accuracy, Inference Time, Model Size\n",
    "4. **Output**: Aggregated results saved to CSV and JSON for charting\n",
    "\n",
    "## Results:\n",
    "- TITLE model performance on TITLE datasets\n",
    "- NO-TITLE model performance on NO-TITLE datasets\n",
    "- Aggregated metrics for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "357ed1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üîß PyTorch version: 2.7.0+cu118\n",
      "üíª Device available: CUDA\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "# Add project root to path for importing custom modules\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import custom model and functions\n",
    "from scripts.train import (\n",
    "    RobertaCRFForTokenClassification, \n",
    "    confidence_based_postprocessing,\n",
    "    LABEL2ID, ID2LABEL, NUM_LABELS\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972a4d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Configuration:\n",
      "  Project Root: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\n",
      "  TITLE Model: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\models\\roberta-finetuned-ner-TITLE-v3\n",
      "  NO-TITLE Model: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\models\\roberta-finetuned-ner-NO-TITLE-v3\n",
      "  TITLE Datasets: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\data\\ds\\TITLE\n",
      "  NO-TITLE Datasets: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\data\\ds\\NO-TITLE\n",
      "  Results Output: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\n",
      "‚úÖ TITLE Model found\n",
      "‚úÖ NO-TITLE Model found\n",
      "‚úÖ TITLE Datasets found\n",
      "‚úÖ NO-TITLE Datasets found\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Paths\n",
    "PROJECT_ROOT = Path(r\"c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\")\n",
    "DATA_DS_PATH = PROJECT_ROOT / \"data\" / \"ds\"\n",
    "MODELS_PATH = PROJECT_ROOT / \"models\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"evaluation_results\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Model paths\n",
    "TITLE_MODEL_PATH = MODELS_PATH / \"roberta-finetuned-ner-TITLE-v3\"\n",
    "NO_TITLE_MODEL_PATH = MODELS_PATH / \"roberta-finetuned-ner-NO-TITLE-v3\"\n",
    "\n",
    "# Dataset paths\n",
    "TITLE_DATASETS_PATH = DATA_DS_PATH / \"TITLE\"\n",
    "NO_TITLE_DATASETS_PATH = DATA_DS_PATH / \"NO-TITLE\"\n",
    "\n",
    "print(\"üìÇ Configuration:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  TITLE Model: {TITLE_MODEL_PATH}\")\n",
    "print(f\"  NO-TITLE Model: {NO_TITLE_MODEL_PATH}\")\n",
    "print(f\"  TITLE Datasets: {TITLE_DATASETS_PATH}\")\n",
    "print(f\"  NO-TITLE Datasets: {NO_TITLE_DATASETS_PATH}\")\n",
    "print(f\"  Results Output: {RESULTS_PATH}\")\n",
    "\n",
    "# Verify paths exist\n",
    "for path, name in [(TITLE_MODEL_PATH, \"TITLE Model\"), (NO_TITLE_MODEL_PATH, \"NO-TITLE Model\"), \n",
    "                   (TITLE_DATASETS_PATH, \"TITLE Datasets\"), (NO_TITLE_DATASETS_PATH, \"NO-TITLE Datasets\")]:\n",
    "    if path.exists():\n",
    "        print(f\"‚úÖ {name} found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} NOT found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70442c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Discovering TITLE datasets...\n",
      "üìä Found dataset: a\n",
      "üìä Found dataset: conllpp_train\n",
      "üìä Found dataset: KAGGLE\n",
      "üìä Found dataset: ritter\n",
      "üìä Found dataset: WNUT\n",
      "\n",
      "üîç Discovering NO-TITLE datasets...\n",
      "üìä Found dataset: a\n",
      "üìä Found dataset: conllpp_train\n",
      "üìä Found dataset: KAGGLE\n",
      "üìä Found dataset: result\n",
      "üìä Found dataset: ritter\n",
      "üìä Found dataset: test\n",
      "üìä Found dataset: WNUT\n",
      "\n",
      "üìà Summary:\n",
      "  TITLE datasets: 5\n",
      "  NO-TITLE datasets: 7\n",
      "  Total datasets: 12\n"
     ]
    }
   ],
   "source": [
    "# Dataset Discovery Function\n",
    "def discover_datasets(datasets_path):\n",
    "    \"\"\"\n",
    "    Discover all transformer datasets in the given directory.\n",
    "    Returns a list of dataset paths.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    datasets_path = Path(datasets_path)\n",
    "    \n",
    "    if not datasets_path.exists():\n",
    "        print(f\"‚ùå Path {datasets_path} does not exist!\")\n",
    "        return datasets\n",
    "    \n",
    "    # Look for directories that contain dataset files\n",
    "    for item in datasets_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            # Check if it's a valid transformers dataset\n",
    "            if (item / \"dataset_dict.json\").exists() or (item / \"dataset_info.json\").exists():\n",
    "                datasets.append(item)\n",
    "                print(f\"üìä Found dataset: {item.name}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Discover all datasets\n",
    "print(\"üîç Discovering TITLE datasets...\")\n",
    "title_datasets = discover_datasets(TITLE_DATASETS_PATH)\n",
    "\n",
    "print(f\"\\nüîç Discovering NO-TITLE datasets...\")\n",
    "no_title_datasets = discover_datasets(NO_TITLE_DATASETS_PATH)\n",
    "\n",
    "print(f\"\\nüìà Summary:\")\n",
    "print(f\"  TITLE datasets: {len(title_datasets)}\")\n",
    "print(f\"  NO-TITLE datasets: {len(no_title_datasets)}\")\n",
    "print(f\"  Total datasets: {len(title_datasets) + len(no_title_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66640937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "üîß Loading TITLE model from c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\models\\roberta-finetuned-ner-TITLE-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded 208 weight tensors\n",
      "  üìä Model size: 484.5 MB\n",
      "üîß Loading NO-TITLE model from c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\models\\roberta-finetuned-ner-NO-TITLE-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Loaded 208 weight tensors\n",
      "  üìä Model size: 484.5 MB\n",
      "\n",
      "‚úÖ Both models loaded successfully!\n",
      "  TITLE model: 5 labels, 484.5 MB\n",
      "  NO-TITLE model: 3 labels, 484.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Model Loading Functions\n",
    "def load_model_and_tokenizer(model_path, model_type=\"NO-TITLE\"):\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer from the given path.\n",
    "    Returns model, tokenizer, label_config\n",
    "    \"\"\"\n",
    "    print(f\"üîß Loading {model_type} model from {model_path}\")\n",
    "    \n",
    "    # Load label configuration\n",
    "    label_config_path = model_path / \"label_config.json\"\n",
    "    with open(label_config_path, 'r') as f:\n",
    "        label_config = json.load(f)\n",
    "    \n",
    "    # Get model configuration from config.json\n",
    "    config_path = model_path / \"config.json\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Create model with exact same architecture as training\n",
    "    model = RobertaCRFForTokenClassification(\n",
    "        model_name=\"roberta-base\",  # Base model\n",
    "        num_labels=label_config['num_labels'],\n",
    "        alpha=config.get('alpha', 0.25),\n",
    "        gamma=config.get('gamma', 2.0),\n",
    "        person_weight=config.get('person_weight', 5.0),\n",
    "        crf_weight=config.get('crf_weight', 0.5),\n",
    "        focal_weight=config.get('focal_weight', 0.2),\n",
    "        dice_weight=config.get('dice_weight', 0.3),\n",
    "        classifier_params=config.get('classifier_params', {}),\n",
    "        dice_loss_params=config.get('dice_loss_params', {})\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    from safetensors.torch import load_file\n",
    "    model_file = model_path / \"model.safetensors\"\n",
    "    if model_file.exists():\n",
    "        state_dict = load_file(str(model_file))\n",
    "        \n",
    "        # Filter compatible weights\n",
    "        model_state_dict = model.state_dict()\n",
    "        compatible_state_dict = {}\n",
    "        \n",
    "        for key, value in state_dict.items():\n",
    "            if key in model_state_dict and model_state_dict[key].shape == value.shape:\n",
    "                compatible_state_dict[key] = value\n",
    "        \n",
    "        model.load_state_dict(compatible_state_dict, strict=False)\n",
    "        print(f\"  ‚úÖ Loaded {len(compatible_state_dict)} weight tensors\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path), add_prefix_space=True)\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "    print(f\"  üìä Model size: {model_size_mb:.1f} MB\")\n",
    "    \n",
    "    return model, tokenizer, label_config, model_size_mb\n",
    "\n",
    "# Load both models\n",
    "print(\"Loading models...\")\n",
    "title_model, title_tokenizer, title_label_config, title_model_size = load_model_and_tokenizer(\n",
    "    TITLE_MODEL_PATH, \"TITLE\"\n",
    ")\n",
    "\n",
    "no_title_model, no_title_tokenizer, no_title_label_config, no_title_model_size = load_model_and_tokenizer(\n",
    "    NO_TITLE_MODEL_PATH, \"NO-TITLE\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Both models loaded successfully!\")\n",
    "print(f\"  TITLE model: {title_label_config['num_labels']} labels, {title_model_size:.1f} MB\")\n",
    "print(f\"  NO-TITLE model: {no_title_label_config['num_labels']} labels, {no_title_model_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1257d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model_on_dataset(model, tokenizer, label_config, dataset_path, model_type):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a single dataset and return metrics.\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_path.name\n",
    "    print(f\"  üî¨ Evaluating on {dataset_name}...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        dataset = load_from_disk(str(dataset_path))\n",
    "        \n",
    "        # Check if dataset has splits and use appropriate data\n",
    "        if hasattr(dataset, 'keys'):\n",
    "            # Dataset has splits, use test if available, otherwise use the first available split\n",
    "            if 'test' in dataset:\n",
    "                eval_data = dataset['test']\n",
    "            elif 'validation' in dataset:\n",
    "                eval_data = dataset['validation']\n",
    "            else:\n",
    "                eval_data = dataset[list(dataset.keys())[0]]\n",
    "        else:\n",
    "            # Single dataset without splits\n",
    "            eval_data = dataset\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Failed to load dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Filter labels to match model\n",
    "    def filter_labels(example):\n",
    "        filtered_labels = []\n",
    "        for label in example['labels']:\n",
    "            if label == -100:\n",
    "                filtered_labels.append(label)\n",
    "            elif label >= label_config['num_labels']:\n",
    "                filtered_labels.append(0)  # Map invalid labels to O\n",
    "            else:\n",
    "                filtered_labels.append(label)\n",
    "        example['labels'] = filtered_labels\n",
    "        return example\n",
    "    \n",
    "    eval_data = eval_data.map(filter_labels)\n",
    "    \n",
    "    # Set up evaluation\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process in batches\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(eval_data, batch_size=8, collate_fn=data_collator)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=None)\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            # Use CRF decoding\n",
    "            try:\n",
    "                crf_mask = attention_mask.bool()\n",
    "                if hasattr(model, 'crf') and hasattr(model.crf, 'decode'):\n",
    "                    crf_predictions = model.crf.decode(logits, mask=crf_mask)\n",
    "                    predictions = torch.zeros_like(input_ids)\n",
    "                    for b_idx, pred_seq in enumerate(crf_predictions):\n",
    "                        seq_len = min(len(pred_seq), predictions.shape[1])\n",
    "                        predictions[b_idx, :seq_len] = torch.tensor(pred_seq[:seq_len])\n",
    "                else:\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "            except:\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Apply post-processing\n",
    "            try:\n",
    "                predictions = confidence_based_postprocessing(logits, predictions, attention_mask)\n",
    "            except:\n",
    "                pass  # Continue with unprocessed predictions\n",
    "            \n",
    "            # Extract sequences for evaluation\n",
    "            for b in range(predictions.shape[0]):\n",
    "                pred_seq = []\n",
    "                label_seq = []\n",
    "                \n",
    "                for t in range(predictions.shape[1]):\n",
    "                    if attention_mask[b, t] == 1 and labels[b, t] != -100:\n",
    "                        pred_id = predictions[b, t].item()\n",
    "                        label_id = labels[b, t].item()\n",
    "                        \n",
    "                        pred_label = label_config['id2label'].get(str(pred_id), \"O\")\n",
    "                        true_label = label_config['id2label'].get(str(label_id), \"O\")\n",
    "                        \n",
    "                        pred_seq.append(pred_label)\n",
    "                        label_seq.append(true_label)\n",
    "                \n",
    "                if pred_seq and label_seq:\n",
    "                    all_predictions.append(pred_seq)\n",
    "                    all_labels.append(label_seq)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if not all_predictions or not all_labels:\n",
    "        print(f\"    ‚ùå No valid predictions for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Entity-level metrics using seqeval\n",
    "    entity_results = seq_classification_report(all_labels, all_predictions, scheme=IOB2, output_dict=True)\n",
    "    \n",
    "    # Token-level metrics\n",
    "    all_true_labels = [l for seq in all_labels for l in seq]\n",
    "    all_pred_labels = [p for seq in all_predictions for p in seq]\n",
    "    \n",
    "    token_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "    \n",
    "    # Get precision, recall, F1 for PERSON entity specifically\n",
    "    person_metrics = entity_results.get(\"PERSON\", {\"precision\": 0.0, \"recall\": 0.0, \"f1-score\": 0.0})\n",
    "    \n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'model_type': model_type,\n",
    "        'num_samples': len(all_predictions),\n",
    "        'inference_time_seconds': inference_time,\n",
    "        'inference_time_per_sample': inference_time / len(all_predictions),\n",
    "        'person_precision': person_metrics[\"precision\"],\n",
    "        'person_recall': person_metrics[\"recall\"],\n",
    "        'person_f1': person_metrics[\"f1-score\"],\n",
    "        'entity_f1_macro': entity_results[\"macro avg\"][\"f1-score\"],\n",
    "        'token_accuracy': token_accuracy,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"    ‚úÖ {dataset_name}: Person F1={results['person_f1']:.3f}, Token Acc={results['token_accuracy']:.3f}, Time={inference_time:.1f}s\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581392e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive evaluation pipeline...\n",
      "============================================================\n",
      "\n",
      "üìä TITLE Model Evaluation\n",
      "------------------------------\n",
      "  üî¨ Evaluating on a...\n",
      "    ‚úÖ a: Person F1=0.332, Token Acc=0.934, Time=43.9s\n",
      "  üî¨ Evaluating on conllpp_train...\n",
      "    ‚úÖ conllpp_train: Person F1=0.820, Token Acc=0.976, Time=66.7s\n",
      "  üî¨ Evaluating on KAGGLE...\n",
      "    ‚úÖ KAGGLE: Person F1=0.533, Token Acc=0.967, Time=227.3s\n",
      "  üî¨ Evaluating on ritter...\n",
      "    ‚úÖ ritter: Person F1=0.857, Token Acc=0.995, Time=11.9s\n",
      "  üî¨ Evaluating on WNUT...\n",
      "    ‚úÖ WNUT: Person F1=0.765, Token Acc=0.989, Time=27.5s\n",
      "\n",
      "üìä NO-TITLE Model Evaluation\n",
      "------------------------------\n",
      "  üî¨ Evaluating on a...\n",
      "    ‚úÖ a: Person F1=0.265, Token Acc=0.931, Time=45.7s\n",
      "  üî¨ Evaluating on conllpp_train...\n",
      "    ‚úÖ conllpp_train: Person F1=0.817, Token Acc=0.981, Time=68.4s\n",
      "  üî¨ Evaluating on KAGGLE...\n",
      "    ‚úÖ KAGGLE: Person F1=0.577, Token Acc=0.973, Time=228.6s\n",
      "  üî¨ Evaluating on result...\n",
      "    ‚úÖ result: Person F1=0.817, Token Acc=0.981, Time=68.4s\n",
      "  üî¨ Evaluating on ritter...\n",
      "    ‚úÖ ritter: Person F1=0.773, Token Acc=0.994, Time=12.0s\n",
      "  üî¨ Evaluating on test...\n",
      "    ‚úÖ test: Person F1=0.817, Token Acc=0.981, Time=68.7s\n",
      "  üî¨ Evaluating on WNUT...\n",
      "    ‚úÖ WNUT: Person F1=0.728, Token Acc=0.988, Time=27.7s\n",
      "\n",
      "‚úÖ Evaluation complete! Processed 12 dataset-model combinations.\n",
      "\n",
      "üìà Results Summary:\n",
      "  Total evaluations: 12\n",
      "  TITLE evaluations: 5\n",
      "  NO-TITLE evaluations: 7\n",
      "\n",
      "üîç Sample Results:\n",
      "      dataset model_type  person_f1  person_precision  person_recall  token_accuracy\n",
      "            a      TITLE   0.331971          0.736842       0.214248        0.933577\n",
      "conllpp_train      TITLE   0.819505          0.771281       0.874161        0.975679\n",
      "       KAGGLE      TITLE   0.532571          0.438516       0.677989        0.966857\n",
      "       ritter      TITLE   0.857143          0.784091       0.945205        0.995314\n",
      "         WNUT      TITLE   0.765306          0.815217       0.721154        0.988896\n",
      "            a   NO-TITLE   0.265464          0.713626       0.163061        0.931310\n",
      "conllpp_train   NO-TITLE   0.816559          0.746010       0.901846        0.980867\n",
      "       KAGGLE   NO-TITLE   0.577231          0.484160       0.714601        0.973175\n",
      "       result   NO-TITLE   0.816559          0.746010       0.901846        0.980867\n",
      "       ritter   NO-TITLE   0.773006          0.700000       0.863014        0.994255\n"
     ]
    }
   ],
   "source": [
    "# Main Evaluation Pipeline\n",
    "print(\"üöÄ Starting comprehensive evaluation pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Evaluate TITLE model on TITLE datasets\n",
    "print(\"\\nüìä TITLE Model Evaluation\")\n",
    "print(\"-\" * 30)\n",
    "for dataset_path in title_datasets:\n",
    "    result = evaluate_model_on_dataset(\n",
    "        title_model, title_tokenizer, title_label_config, \n",
    "        dataset_path, \"TITLE\"\n",
    "    )\n",
    "    if result:\n",
    "        result['model_size_mb'] = title_model_size\n",
    "        all_results.append(result)\n",
    "\n",
    "# Evaluate NO-TITLE model on NO-TITLE datasets  \n",
    "print(\"\\nüìä NO-TITLE Model Evaluation\")\n",
    "print(\"-\" * 30)\n",
    "for dataset_path in no_title_datasets:\n",
    "    result = evaluate_model_on_dataset(\n",
    "        no_title_model, no_title_tokenizer, no_title_label_config,\n",
    "        dataset_path, \"NO-TITLE\"\n",
    "    )\n",
    "    if result:\n",
    "        result['model_size_mb'] = no_title_model_size\n",
    "        all_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete! Processed {len(all_results)} dataset-model combinations.\")\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(f\"\\nüìà Results Summary:\")\n",
    "print(f\"  Total evaluations: {len(df_results)}\")\n",
    "print(f\"  TITLE evaluations: {len(df_results[df_results['model_type'] == 'TITLE'])}\")\n",
    "print(f\"  NO-TITLE evaluations: {len(df_results[df_results['model_type'] == 'NO-TITLE'])}\")\n",
    "\n",
    "# Display first few results\n",
    "if len(df_results) > 0:\n",
    "    print(f\"\\nüîç Sample Results:\")\n",
    "    display_cols = ['dataset', 'model_type', 'person_f1', 'person_precision', 'person_recall', 'token_accuracy']\n",
    "    print(df_results[display_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "298d473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aggregating Results by Model Type\n",
      "==================================================\n",
      "\n",
      "üîç TITLE Model Summary:\n",
      "  Datasets evaluated: 5\n",
      "  Total samples: 8,038\n",
      "  Model size: 484.5 MB\n",
      "  Total inference time: 377.3s\n",
      "  Avg time per sample: 47.4ms\n",
      "  Person F1: 0.661 ¬± 0.223 (range: 0.332 - 0.857)\n",
      "  Person Precision: 0.709 ¬± 0.154\n",
      "  Person Recall: 0.687 ¬± 0.286\n",
      "  Token Accuracy: 0.972 ¬± 0.024\n",
      "\n",
      "üîç NO-TITLE Model Summary:\n",
      "  Datasets evaluated: 7\n",
      "  Total samples: 11,036\n",
      "  Model size: 484.5 MB\n",
      "  Total inference time: 519.5s\n",
      "  Avg time per sample: 47.5ms\n",
      "  Person F1: 0.685 ¬± 0.204 (range: 0.265 - 0.817)\n",
      "  Person Precision: 0.703 ¬± 0.101\n",
      "  Person Recall: 0.732 ¬± 0.268\n",
      "  Token Accuracy: 0.976 ¬± 0.021\n",
      "\n",
      "üÜö Model Comparison:\n",
      "  F1 Score: TITLE(0.661) vs NO-TITLE(0.685)\n",
      "  Precision: TITLE(0.709) vs NO-TITLE(0.703)\n",
      "  Recall: TITLE(0.687) vs NO-TITLE(0.732)\n",
      "  Speed: TITLE(47.4ms) vs NO-TITLE(47.5ms)\n",
      "  Size: TITLE(484.5MB) vs NO-TITLE(484.5MB)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Results by Model Type\n",
    "print(\"\\nüìä Aggregating Results by Model Type\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Aggregate metrics by model type\n",
    "    aggregated_results = []\n",
    "    \n",
    "    for model_type in ['TITLE', 'NO-TITLE']:\n",
    "        model_data = df_results[df_results['model_type'] == model_type]\n",
    "        \n",
    "        if len(model_data) > 0:\n",
    "            # Calculate aggregate statistics\n",
    "            agg_result = {\n",
    "                'model_type': model_type,\n",
    "                'num_datasets': len(model_data),\n",
    "                'total_samples': model_data['num_samples'].sum(),\n",
    "                'total_inference_time': model_data['inference_time_seconds'].sum(),\n",
    "                'avg_inference_time_per_sample': model_data['inference_time_per_sample'].mean(),\n",
    "                'model_size_mb': model_data['model_size_mb'].iloc[0],  # Same for all entries\n",
    "                \n",
    "                # Person entity metrics\n",
    "                'person_f1_mean': model_data['person_f1'].mean(),\n",
    "                'person_f1_std': model_data['person_f1'].std(),\n",
    "                'person_f1_min': model_data['person_f1'].min(),\n",
    "                'person_f1_max': model_data['person_f1'].max(),\n",
    "                \n",
    "                'person_precision_mean': model_data['person_precision'].mean(),\n",
    "                'person_precision_std': model_data['person_precision'].std(),\n",
    "                \n",
    "                'person_recall_mean': model_data['person_recall'].mean(),\n",
    "                'person_recall_std': model_data['person_recall'].std(),\n",
    "                \n",
    "                # Overall metrics\n",
    "                'entity_f1_macro_mean': model_data['entity_f1_macro'].mean(),\n",
    "                'entity_f1_macro_std': model_data['entity_f1_macro'].std(),\n",
    "                \n",
    "                'token_accuracy_mean': model_data['token_accuracy'].mean(),\n",
    "                'token_accuracy_std': model_data['token_accuracy'].std(),\n",
    "                \n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            aggregated_results.append(agg_result)\n",
    "            \n",
    "            # Print summary for this model type\n",
    "            print(f\"\\nüîç {model_type} Model Summary:\")\n",
    "            print(f\"  Datasets evaluated: {agg_result['num_datasets']}\")\n",
    "            print(f\"  Total samples: {agg_result['total_samples']:,}\")\n",
    "            print(f\"  Model size: {agg_result['model_size_mb']:.1f} MB\")\n",
    "            print(f\"  Total inference time: {agg_result['total_inference_time']:.1f}s\")\n",
    "            print(f\"  Avg time per sample: {agg_result['avg_inference_time_per_sample']*1000:.1f}ms\")\n",
    "            print(f\"  Person F1: {agg_result['person_f1_mean']:.3f} ¬± {agg_result['person_f1_std']:.3f} (range: {agg_result['person_f1_min']:.3f} - {agg_result['person_f1_max']:.3f})\")\n",
    "            print(f\"  Person Precision: {agg_result['person_precision_mean']:.3f} ¬± {agg_result['person_precision_std']:.3f}\")\n",
    "            print(f\"  Person Recall: {agg_result['person_recall_mean']:.3f} ¬± {agg_result['person_recall_std']:.3f}\")\n",
    "            print(f\"  Token Accuracy: {agg_result['token_accuracy_mean']:.3f} ¬± {agg_result['token_accuracy_std']:.3f}\")\n",
    "    \n",
    "    # Create aggregated DataFrame\n",
    "    df_aggregated = pd.DataFrame(aggregated_results)\n",
    "    \n",
    "    # Comparison summary\n",
    "    if len(df_aggregated) == 2:\n",
    "        print(f\"\\nüÜö Model Comparison:\")\n",
    "        title_row = df_aggregated[df_aggregated['model_type'] == 'TITLE'].iloc[0]\n",
    "        no_title_row = df_aggregated[df_aggregated['model_type'] == 'NO-TITLE'].iloc[0]\n",
    "        \n",
    "        print(f\"  F1 Score: TITLE({title_row['person_f1_mean']:.3f}) vs NO-TITLE({no_title_row['person_f1_mean']:.3f})\")\n",
    "        print(f\"  Precision: TITLE({title_row['person_precision_mean']:.3f}) vs NO-TITLE({no_title_row['person_precision_mean']:.3f})\")\n",
    "        print(f\"  Recall: TITLE({title_row['person_recall_mean']:.3f}) vs NO-TITLE({no_title_row['person_recall_mean']:.3f})\")\n",
    "        print(f\"  Speed: TITLE({title_row['avg_inference_time_per_sample']*1000:.1f}ms) vs NO-TITLE({no_title_row['avg_inference_time_per_sample']*1000:.1f}ms)\")\n",
    "        print(f\"  Size: TITLE({title_row['model_size_mb']:.1f}MB) vs NO-TITLE({no_title_row['model_size_mb']:.1f}MB)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No results to aggregate!\")\n",
    "    df_aggregated = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving Results to Files\n",
      "==============================\n",
      "‚úÖ Detailed results saved to: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\\detailed_evaluation_results_20250611_010923.csv\n",
      "‚úÖ Detailed results saved to: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\\detailed_evaluation_results_20250611_010923.json\n",
      "‚úÖ Aggregated results saved to: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\\aggregated_evaluation_results_20250611_010923.csv\n",
      "‚úÖ Aggregated results saved to: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\\aggregated_evaluation_results_20250611_010923.json\n",
      "‚úÖ Summary report saved to: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\\evaluation_summary_20250611_010923.txt\n",
      "\\nüìÅ All generated files:\n",
      "  aggregated_evaluation_results_20250611_010923.csv\n",
      "  aggregated_evaluation_results_20250611_010923.json\n",
      "  detailed_evaluation_results_20250611_010923.csv\n",
      "  detailed_evaluation_results_20250611_010923.json\n",
      "  evaluation_summary_20250611_010923.txt\n",
      "\\nüéâ Evaluation pipeline completed successfully!\n",
      "üìä Results available in: c:\\Users\\Administrator\\Desktop\\tmp\\NER-proper-names\\evaluation_results\n"
     ]
    }
   ],
   "source": [
    "# Save Results to Files\n",
    "print(\"\\nüíæ Saving Results to Files\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Save detailed results only (aggregated can be calculated when loading)\n",
    "    detailed_csv_path = RESULTS_PATH / f\"detailed_evaluation_results_{timestamp}.csv\"\n",
    "    detailed_json_path = RESULTS_PATH / f\"detailed_evaluation_results_{timestamp}.json\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_results.to_csv(detailed_csv_path, index=False)\n",
    "    print(f\"‚úÖ Detailed results saved to: {detailed_csv_path}\")\n",
    "    \n",
    "    # Save to JSON (with better formatting)\n",
    "    df_results.to_json(detailed_json_path, orient='records', indent=2)\n",
    "    print(f\"‚úÖ Detailed results saved to: {detailed_json_path}\")\n",
    "    \n",
    "    # Save a summary report\n",
    "    summary_path = RESULTS_PATH / f\"evaluation_summary_{timestamp}.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"NER Model Evaluation Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Evaluations: {len(df_results)}\\n\")\n",
    "        f.write(f\"TITLE Evaluations: {len(df_results[df_results['model_type'] == 'TITLE'])}\\n\")\n",
    "        f.write(f\"NO-TITLE Evaluations: {len(df_results[df_results['model_type'] == 'NO-TITLE'])}\\n\\n\")\n",
    "        \n",
    "        # Write individual dataset results summary\n",
    "        f.write(\"Individual Dataset Results:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for _, row in df_results.iterrows():\n",
    "            f.write(f\"{row['dataset']} ({row['model_type']}):\\n\")\n",
    "            f.write(f\"  Person F1: {row['person_f1']:.3f}\\n\")\n",
    "            f.write(f\"  Person Precision: {row['person_precision']:.3f}\\n\")\n",
    "            f.write(f\"  Person Recall: {row['person_recall']:.3f}\\n\")\n",
    "            f.write(f\"  Token Accuracy: {row['token_accuracy']:.3f}\\n\")\n",
    "            f.write(f\"  Inference Time: {row['inference_time_per_sample']*1000:.1f}ms per sample\\n\\n\")\n",
    "        \n",
    "        # Write aggregated statistics (calculated on-the-fly)\n",
    "        if len(df_aggregated) > 0:\n",
    "            f.write(\"\\nAggregated Statistics (calculated from detailed results):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            for _, row in df_aggregated.iterrows():\n",
    "                f.write(f\"{row['model_type']} Model Summary:\\n\")\n",
    "                f.write(f\"  Datasets: {row['num_datasets']}\\n\")\n",
    "                f.write(f\"  Samples: {row['total_samples']:,}\\n\")\n",
    "                f.write(f\"  Person F1: {row['person_f1_mean']:.3f} ¬± {row['person_f1_std']:.3f}\\n\")\n",
    "                f.write(f\"  Person Precision: {row['person_precision_mean']:.3f} ¬± {row['person_precision_std']:.3f}\\n\")\n",
    "                f.write(f\"  Person Recall: {row['person_recall_mean']:.3f} ¬± {row['person_recall_std']:.3f}\\n\")\n",
    "                f.write(f\"  Token Accuracy: {row['token_accuracy_mean']:.3f} ¬± {row['token_accuracy_std']:.3f}\\n\")\n",
    "                f.write(f\"  Model Size: {row['model_size_mb']:.1f} MB\\n\")\n",
    "                f.write(f\"  Avg Inference Time: {row['avg_inference_time_per_sample']*1000:.1f}ms per sample\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Summary report saved to: {summary_path}\")\n",
    "    \n",
    "    # Display file list\n",
    "    print(f\"\\nüìÅ Generated files:\")\n",
    "    for file_path in RESULTS_PATH.glob(f\"*{timestamp}*\"):\n",
    "        print(f\"  {file_path.name}\")\n",
    "        \n",
    "    print(f\"\\nüí° Note: Only detailed results are saved to files.\")\n",
    "    print(f\"   Aggregated statistics can be recalculated when loading the detailed CSV/JSON.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No results to save!\")\n",
    "\n",
    "print(f\"\\nüéâ Evaluation pipeline completed successfully!\")\n",
    "print(f\"üìä Results available in: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12bde513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüìà Data Preview for Charting\n",
      "===================================\n",
      "\\nüîç Detailed Results Preview:\n",
      "Columns: ['dataset', 'model_type', 'num_samples', 'inference_time_seconds', 'inference_time_per_sample', 'person_precision', 'person_recall', 'person_f1', 'entity_f1_macro', 'token_accuracy', 'timestamp', 'model_size_mb']\n",
      "\\nSample data:\n",
      "         dataset model_type  person_f1  person_precision  person_recall  \\\n",
      "0              a      TITLE   0.331971          0.736842       0.214248   \n",
      "1  conllpp_train      TITLE   0.819505          0.771281       0.874161   \n",
      "2         KAGGLE      TITLE   0.532571          0.438516       0.677989   \n",
      "3         ritter      TITLE   0.857143          0.784091       0.945205   \n",
      "4           WNUT      TITLE   0.765306          0.815217       0.721154   \n",
      "\n",
      "   token_accuracy  inference_time_per_sample  model_size_mb  \n",
      "0        0.933577                   0.046962     484.540474  \n",
      "1        0.975679                   0.044524     484.540474  \n",
      "2        0.966857                   0.047391     484.540474  \n",
      "3        0.995314                   0.049613     484.540474  \n",
      "4        0.988896                   0.048380     484.540474  \n",
      "\\n\\nüîç Aggregated Results Preview:\n",
      "Columns: ['model_type', 'num_datasets', 'total_samples', 'total_inference_time', 'avg_inference_time_per_sample', 'model_size_mb', 'person_f1_mean', 'person_f1_std', 'person_f1_min', 'person_f1_max', 'person_precision_mean', 'person_precision_std', 'person_recall_mean', 'person_recall_std', 'entity_f1_macro_mean', 'entity_f1_macro_std', 'token_accuracy_mean', 'token_accuracy_std', 'timestamp']\n",
      "\\nData:\n",
      "  model_type  num_datasets  person_f1_mean  person_f1_std  \\\n",
      "0      TITLE             5        0.661299       0.223176   \n",
      "1   NO-TITLE             7        0.684709       0.203759   \n",
      "\n",
      "   person_precision_mean  person_recall_mean  token_accuracy_mean  \\\n",
      "0               0.709189            0.686551             0.972065   \n",
      "1               0.703304            0.731785             0.975661   \n",
      "\n",
      "   model_size_mb  avg_inference_time_per_sample  \n",
      "0     484.540474                       0.047374  \n",
      "1     484.528656                       0.047462  \n",
      "\\nüí° Ready for Charting!\n",
      "   Use 'df_results' for detailed dataset-level analysis\n",
      "   Use 'df_aggregated' for model-level comparisons\n",
      "   Data also saved as CSV files for external tools\n",
      "\\nüèÅ Pipeline Complete! Ready for visualization and analysis.\n"
     ]
    }
   ],
   "source": [
    "# Quick Data Preview for Charting\n",
    "print(\"\\\\nüìà Data Preview for Charting\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    print(\"\\\\nüîç Detailed Results Preview:\")\n",
    "    print(\"Columns:\", list(df_results.columns))\n",
    "    print(\"\\\\nSample data:\")\n",
    "    preview_cols = ['dataset', 'model_type', 'person_f1', 'person_precision', 'person_recall', \n",
    "                   'token_accuracy', 'inference_time_per_sample', 'model_size_mb']\n",
    "    print(df_results[preview_cols].head())\n",
    "    \n",
    "    if len(df_aggregated) > 0:\n",
    "        print(\"\\\\n\\\\nüîç Aggregated Results Preview:\")\n",
    "        print(\"Columns:\", list(df_aggregated.columns))\n",
    "        print(\"\\\\nData:\")\n",
    "        agg_preview_cols = ['model_type', 'num_datasets', 'person_f1_mean', 'person_f1_std',\n",
    "                           'person_precision_mean', 'person_recall_mean', 'token_accuracy_mean',\n",
    "                           'model_size_mb', 'avg_inference_time_per_sample']\n",
    "        print(df_aggregated[agg_preview_cols])\n",
    "        \n",
    "    print(f\"\\\\nüí° Ready for Charting!\")\n",
    "    print(f\"   Use 'df_results' for detailed dataset-level analysis\")\n",
    "    print(f\"   Use 'df_aggregated' for model-level comparisons\")\n",
    "    print(f\"   Data also saved as CSV files for external tools\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for charting!\")\n",
    "\n",
    "print(f\"\\\\nüèÅ Pipeline Complete! Ready for visualization and analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
