
# NER Model Evaluation Summary Report
Generated: 2025-06-11 10:58:30

## Overall Results

### Dataset Coverage:
- Total evaluations: 8
- Unique datasets: 4
- Model types evaluated: TITLE, NO-TITLE

### Key Findings:


#### TITLE Model Performance:
- **F1 Score**: 0.556 ± 0.296 (range: 0.245 - 0.884)
- **Precision**: 0.474 ± 0.324
- **Recall**: 0.799 ± 0.119
- **Token Accuracy**: 0.958 ± 0.025
- **Model Size**: 484.5 MB
- **Avg Inference Time**: 44.5 ms per sample
- **Datasets Evaluated**: 4
- **Total Samples**: 7,104

#### NO-TITLE Model Performance:
- **F1 Score**: 0.584 ± 0.284 (range: 0.284 - 0.894)
- **Precision**: 0.512 ± 0.325
- **Recall**: 0.782 ± 0.130
- **Token Accuracy**: 0.969 ± 0.022
- **Model Size**: 484.5 MB
- **Avg Inference Time**: 46.5 ms per sample
- **Datasets Evaluated**: 4
- **Total Samples**: 7,104

### Best Performing Datasets by Model:

**TITLE Model - Best Dataset: conllpp_train**
- F1 Score: 0.884
- Precision: 0.838
- Recall: 0.935
- Samples: 1,499

**NO-TITLE Model - Best Dataset: conllpp_train**
- F1 Score: 0.894
- Precision: 0.876
- Recall: 0.914
- Samples: 1,499

### Model Comparison Summary:
- **Better F1 Performance**: NO-TITLE model (0.028 points difference)
- **Faster Inference**: TITLE model (1.9 ms difference)
- **Model Sizes**: Nearly identical (~484.5 MB)

### Recommendations:
- For highest accuracy: Use NO-TITLE model
- For fastest inference: Use TITLE model
- Both models show similar efficiency in terms of size/performance ratio

### Generated Visualizations:
The following charts have been saved to the charts directory:
1. Performance Comparison Chart
2. Aggregated Model Comparison
3. Efficiency Analysis (Speed vs Performance)
4. Multi-dimensional Radar Chart
5. Distribution Analysis
6. Performance Heatmap
7. Statistical Analysis with Confidence Intervals
8. Comprehensive Summary Dashboard

All charts are available in both HTML (interactive) and PNG (static) formats.
